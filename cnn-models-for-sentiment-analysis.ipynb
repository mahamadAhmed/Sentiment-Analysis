{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":495.102744,"end_time":"2024-04-26T14:24:51.936971","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-04-26T14:16:36.834227","version":"2.3.2"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.plotting import plot_confusion_matrix\nimport matplotlib.cm as cm\nfrom matplotlib import rcParams\nfrom collections import Counter\nfrom nltk.tokenize import RegexpTokenizer\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.layers import MaxPooling1D\n\n# Download NLTK resources\nnltk.download('stopwords')\nnltk.download('wordnet', \"/kaggle/working/nltk_data/\")\nnltk.download('omw-1.4', \"/kaggle/working/nltk_data/\")\n! unzip /kaggle/working/nltk_data/corpora/wordnet.zip -d /kaggle/working/nltk_data/corpora\n! unzip /kaggle/working/nltk_data/corpora/omw-1.4.zip -d /kaggle/working/nltk_data/corpora\nnltk.data.path.append(\"/kaggle/working/nltk_data/\")\n\n# Load dataset\ndataa = pd.read_csv(\"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\", encoding=\"ISO-8859-1\", engine=\"python\",\n                   names=[\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"])\n","metadata":{"papermill":{"duration":0.383336,"end_time":"2024-04-26T14:24:48.253037","exception":false,"start_time":"2024-04-26T14:24:47.869701","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-04-29T16:04:59.950298Z","iopub.execute_input":"2024-04-29T16:04:59.950665Z","iopub.status.idle":"2024-04-29T16:05:18.888813Z","shell.execute_reply.started":"2024-04-29T16:04:59.950635Z","shell.execute_reply":"2024-04-29T16:05:18.887920Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /kaggle/working/nltk_data/...\n[nltk_data] Downloading package omw-1.4 to\n[nltk_data]     /kaggle/working/nltk_data/...\nArchive:  /kaggle/working/nltk_data/corpora/wordnet.zip\n   creating: /kaggle/working/nltk_data/corpora/wordnet/\n  inflating: /kaggle/working/nltk_data/corpora/wordnet/lexnames  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/data.verb  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.adv  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.verb  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/data.adj  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.adj  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/README  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.sense  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/data.noun  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/data.adv  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/index.noun  \n  inflating: /kaggle/working/nltk_data/corpora/wordnet/adj.exc  \nArchive:  /kaggle/working/nltk_data/corpora/omw-1.4.zip\n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/\n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/fin/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fin/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fin/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fin/wn-data-fin.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/heb/wn-data-heb.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slv/wn-data-slv.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/ita/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ita/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ita/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ita/wn-data-ita.tab  \n extracting: /kaggle/working/nltk_data/corpora/omw-1.4/ita/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/wn-data-nno.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nor/wn-data-nob.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/als/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/als/wn-data-als.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/als/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/als/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/als/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/pol/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/pol/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/pol/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/pol/wn-data-pol.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/\n extracting: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/wn-data-hrv.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/hrv/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/citation.bib  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/wn-data-ita.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/iwn/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/nld/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nld/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nld/wn-data-nld.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/nld/citation.bib  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/wn-data-ron.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ron/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/arb/wn-data-arb.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/isl/wn-data-isl.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/swe/wn-data-swe.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/por/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/por/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/por/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/por/wn-data-por.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/por/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/cow/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/cow/wn-data-cmn.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/cow/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/cow/citation.bib  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/README  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/jpn/wn-data-jpn.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/dan/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/dan/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/dan/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/dan/wn-data-dan.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/wn-data-slk.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/wn-data-lit.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/slk/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/wn-data-bul.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/bul/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-eus.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-cat.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-glg.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/mcr/wn-data-spa.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/ell/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ell/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ell/wn-data-ell.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/ell/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/wn-data-zsm.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/wn-data-ind.tab  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/msa/README  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/fra/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fra/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fra/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/fra/wn-data-fra.tab  \n   creating: /kaggle/working/nltk_data/corpora/omw-1.4/tha/\n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/tha/LICENSE  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/tha/citation.bib  \n  inflating: /kaggle/working/nltk_data/corpora/omw-1.4/tha/wn-data-tha.tab  \n","output_type":"stream"}]},{"cell_type":"code","source":"#ASSIGNING 1 TO POSITIVE SENTIMENT 4\ndata=dataa\ndata.loc[data['label'] == 4, 'label'] = 1\n# Filter the original dataset to separate positive and negative tweets\ndata_pos = data[data['label'] == 1]\ndata_neg = data[data['label'] == 0]\n\n# Take only the first 10,000 tweets\ndata_pos = data_pos.head(10000)\ndata_neg = data_neg.head(10000)\n\n# Concatenate positive and negative tweets\ndata = pd.concat([data_pos, data_neg])","metadata":{"execution":{"iopub.status.busy":"2024-04-29T16:05:46.415756Z","iopub.execute_input":"2024-04-29T16:05:46.416492Z","iopub.status.idle":"2024-04-29T16:05:46.595716Z","shell.execute_reply.started":"2024-04-29T16:05:46.416456Z","shell.execute_reply":"2024-04-29T16:05:46.594921Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data.head","metadata":{"execution":{"iopub.status.busy":"2024-04-29T16:06:30.921962Z","iopub.execute_input":"2024-04-29T16:06:30.922621Z","iopub.status.idle":"2024-04-29T16:06:30.935787Z","shell.execute_reply.started":"2024-04-29T16:06:30.922582Z","shell.execute_reply":"2024-04-29T16:06:30.934877Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<bound method NDFrame.head of         label        time                          date     query  \\\n800000      1  1467822272  Mon Apr 06 22:22:45 PDT 2009  NO_QUERY   \n800001      1  1467822273  Mon Apr 06 22:22:45 PDT 2009  NO_QUERY   \n800002      1  1467822283  Mon Apr 06 22:22:46 PDT 2009  NO_QUERY   \n800003      1  1467822287  Mon Apr 06 22:22:46 PDT 2009  NO_QUERY   \n800004      1  1467822293  Mon Apr 06 22:22:46 PDT 2009  NO_QUERY   \n...       ...         ...                           ...       ...   \n9995        0  1550729779  Sat Apr 18 07:05:12 PDT 2009  NO_QUERY   \n9996        0  1550730633  Sat Apr 18 07:05:23 PDT 2009  NO_QUERY   \n9997        0  1550731192  Sat Apr 18 07:05:29 PDT 2009  NO_QUERY   \n9998        0  1550731281  Sat Apr 18 07:05:30 PDT 2009  NO_QUERY   \n9999        0  1550731500  Sat Apr 18 07:05:32 PDT 2009  NO_QUERY   \n\n             username                                               text  \n800000          ersle       I LOVE @Health4UandPets u guys r the best!!   \n800001       becca210  im meeting up with one of my besties tonight! ...  \n800002      Wingman29  @DaRealSunisaKim Thanks for the Twitter add, S...  \n800003      katarinka  Being sick can be really cheap when it hurts t...  \n800004    _EmilyYoung    @LovesBrooklyn2 he has that effect on everyone   \n...               ...                                                ...  \n9995    thedoyleswife                                    Aww that's sad   \n9996      gia_revenge    stupid dvds stuffing up the good bits in jaws.   \n9997        matmurray  @Dandy_Sephy No. Only close friends and family...  \n9998      lexabuckets  CRAP! After looking when I last tweeted... WHY...  \n9999      AmberKarley                          Its Another Rainboot day   \n\n[20000 rows x 6 columns]>"},"metadata":{}}]},{"cell_type":"code","source":"\n# Preprocessing\ndef preprocess_text(text):\n    text = text.lower()  # Convert text to lowercase\n    text = re.sub('@[^\\s]+', ' ', text)  # Remove emails\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', text)  # Remove URLs\n    text = re.sub('[0-9]+', '', text)  # Remove numbers\n    text = re.sub(r'(.)\\1+', r'\\1', text)  # Remove repeating characters\n    translator = str.maketrans('', '', string.punctuation)  # Remove punctuations\n    text = text.translate(translator)\n    return text\n\nstopwords_list = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in stopwords_list])\n\nstemmer = PorterStemmer()\ndef stem_text(text):\n    return [stemmer.stem(word) for word in text]\n\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(word) for word in text]\n\ntokenizer = RegexpTokenizer(r'\\w+')\n\ndata['text'] = data['text'].apply(preprocess_text)\ndata['text'] = data['text'].apply(remove_stopwords)\ndata['text'] = data['text'].apply(tokenizer.tokenize)\ndata['text'] = data['text'].apply(stem_text)\ndata['text'] = data['text'].apply(lemmatize_text)\n\n# Prepare data for model input\nX = data['text']\ny = data['label']\n\nmax_len = 500\ntok = Tokenizer(num_words=2000)\ntok.fit_on_texts(X)\nsequences = tok.texts_to_sequences(X)\nsequences_matrix = pad_sequences(sequences, maxlen=max_len)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T16:06:36.288508Z","iopub.execute_input":"2024-04-29T16:06:36.288900Z","iopub.status.idle":"2024-04-29T16:06:44.967352Z","shell.execute_reply.started":"2024-04-29T16:06:36.288849Z","shell.execute_reply":"2024-04-29T16:06:44.966525Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"\n# Splitting data\nX_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)\n# Define CNN Model 1\ndef cnn_model_1(max_len=500): \n    inputs = Input(name='inputs', shape=[max_len])\n    layer = Embedding(2000, 50, input_length=max_len)(inputs)\n    layer = Conv1D(64, 3, activation='relu')(layer)\n    layer = GlobalMaxPooling1D()(layer)\n    layer = Dense(128, activation='relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1, activation='sigmoid')(layer)\n    model = Model(inputs=inputs, outputs=layer)\n    model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n    return model\n\nmodel_1 = cnn_model_1()\nhistory_1 = model_1.fit(X_train, Y_train, batch_size=80, epochs=6, validation_split=0.1)\n\naccr_1 = model_1.evaluate(X_test, Y_test)\nprint('CNN Model 1 - Test set\\n  Accuracy: {:0.2f}'.format(accr_1[1]))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T16:22:48.216384Z","iopub.execute_input":"2024-04-29T16:22:48.217107Z","iopub.status.idle":"2024-04-29T16:27:03.890155Z","shell.execute_reply.started":"2024-04-29T16:22:48.217061Z","shell.execute_reply":"2024-04-29T16:27:03.889221Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 309ms/step - accuracy: 0.5668 - loss: 0.6779 - val_accuracy: 0.6936 - val_loss: 0.5797\nEpoch 2/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 268ms/step - accuracy: 0.7534 - loss: 0.5183 - val_accuracy: 0.7336 - val_loss: 0.5436\nEpoch 3/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 263ms/step - accuracy: 0.7903 - loss: 0.4630 - val_accuracy: 0.7286 - val_loss: 0.5489\nEpoch 4/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 257ms/step - accuracy: 0.8169 - loss: 0.4210 - val_accuracy: 0.7179 - val_loss: 0.5580\nEpoch 5/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 250ms/step - accuracy: 0.8413 - loss: 0.3817 - val_accuracy: 0.7143 - val_loss: 0.5725\nEpoch 6/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 228ms/step - accuracy: 0.8750 - loss: 0.3255 - val_accuracy: 0.7107 - val_loss: 0.5999\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7155 - loss: 0.6109\nCNN Model 1 - Test set\n  Accuracy: 0.72\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# CNN Model 2\ndef cnn_model_2(max_len=500): \n    inputs = Input(name='inputs', shape=[max_len])\n    layer = Embedding(2000, 50, input_length=max_len)(inputs)\n    layer = Conv1D(128, 5, activation='relu')(layer)\n    layer = MaxPooling1D(2)(layer)\n    layer = Conv1D(64, 5, activation='relu')(layer)\n    layer = GlobalMaxPooling1D()(layer)\n    layer = Dense(256, activation='relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1, activation='sigmoid')(layer)\n    model = Model(inputs=inputs, outputs=layer)\n    model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n    return model\n\n\nmodel_2 = cnn_model_2()\nhistory_2 = model_2.fit(X_train, Y_train, batch_size=80, epochs=6, validation_split=0.1)\naccr_2 = model_2.evaluate(X_test, Y_test)\nprint('CNN Model 2 - Test set\\n  Accuracy: {:0.2f}'.format(accr_2[1]))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T16:27:58.600444Z","iopub.execute_input":"2024-04-29T16:27:58.600837Z","iopub.status.idle":"2024-04-29T16:30:37.445759Z","shell.execute_reply.started":"2024-04-29T16:27:58.600806Z","shell.execute_reply":"2024-04-29T16:30:37.444822Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 290ms/step - accuracy: 0.5199 - loss: 0.6912 - val_accuracy: 0.6500 - val_loss: 0.6186\nEpoch 2/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 140ms/step - accuracy: 0.7257 - loss: 0.5516 - val_accuracy: 0.7371 - val_loss: 0.5387\nEpoch 3/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 192ms/step - accuracy: 0.7886 - loss: 0.4623 - val_accuracy: 0.7093 - val_loss: 0.5605\nEpoch 4/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 134ms/step - accuracy: 0.8170 - loss: 0.4171 - val_accuracy: 0.7036 - val_loss: 0.6196\nEpoch 5/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 95ms/step - accuracy: 0.8504 - loss: 0.3537 - val_accuracy: 0.7029 - val_loss: 0.6088\nEpoch 6/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 71ms/step - accuracy: 0.8957 - loss: 0.2715 - val_accuracy: 0.6979 - val_loss: 0.7040\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6937 - loss: 0.6900\nCNN Model 2 - Test set\n  Accuracy: 0.70\n","output_type":"stream"}]},{"cell_type":"code","source":"# CNN Model 3\ndef cnn_model_3(max_len=500): \n    inputs = Input(name='inputs', shape=[max_len])\n    layer = Embedding(2000, 50, input_length=max_len)(inputs)\n    layer = Conv1D(64, 3, activation='relu')(layer)\n    layer = MaxPooling1D(2)(layer)\n    layer = Conv1D(64, 3, activation='relu')(layer)\n    layer = GlobalMaxPooling1D()(layer)\n    layer = Dense(128, activation='relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1, activation='sigmoid')(layer)\n    model = Model(inputs=inputs, outputs=layer)\n    model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n    return model\n\n\nmodel_3 = cnn_model_3()\nhistory_3 = model_3.fit(X_train, Y_train, batch_size=80, epochs=6, validation_split=0.1)\naccr_3 = model_3.evaluate(X_test, Y_test)\nprint('CNN Model 3 - Test set\\n  Accuracy: {:0.2f}'.format(accr_3[1]))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T16:30:49.083977Z","iopub.execute_input":"2024-04-29T16:30:49.084680Z","iopub.status.idle":"2024-04-29T16:34:47.789810Z","shell.execute_reply.started":"2024-04-29T16:30:49.084647Z","shell.execute_reply":"2024-04-29T16:34:47.788839Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch 1/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 321ms/step - accuracy: 0.5409 - loss: 0.6863 - val_accuracy: 0.7050 - val_loss: 0.5850\nEpoch 2/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 259ms/step - accuracy: 0.7371 - loss: 0.5393 - val_accuracy: 0.7286 - val_loss: 0.5402\nEpoch 3/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 247ms/step - accuracy: 0.7789 - loss: 0.4759 - val_accuracy: 0.7300 - val_loss: 0.5419\nEpoch 4/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 225ms/step - accuracy: 0.8125 - loss: 0.4241 - val_accuracy: 0.7100 - val_loss: 0.5964\nEpoch 5/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 209ms/step - accuracy: 0.8452 - loss: 0.3760 - val_accuracy: 0.7071 - val_loss: 0.5966\nEpoch 6/6\n\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 185ms/step - accuracy: 0.8635 - loss: 0.3293 - val_accuracy: 0.7071 - val_loss: 0.6632\n\u001b[1m188/188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7175 - loss: 0.6457\nCNN Model 3 - Test set\n  Accuracy: 0.71\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}